{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57e78bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 211225\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all parts\n",
    "df1 = pd.read_csv(\"./Dataset/goemotions1.csv\")\n",
    "df2 = pd.read_csv(\"./Dataset/goemotions2.csv\")\n",
    "df3 = pd.read_csv(\"./Dataset/goemotions3.csv\")\n",
    "\n",
    "# Combine them\n",
    "full_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Display dataset size\n",
    "print(f\"Total samples: {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8f6545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
      "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "3                                 Man I love reddit.  eeibobj   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
      "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.548084e+09        37                  True           0  ...     0   \n",
      "2  1.546428e+09        37                 False           0  ...     0   \n",
      "3  1.547965e+09        18                 False           0  ...     1   \n",
      "4  1.546669e+09         2                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
      "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
      "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
      "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
      "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
      "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
      "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(full_df.head())\n",
    "print(full_df.columns)  # List all column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49d8334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  admiration  amusement  \\\n",
      "0                                    That game hurt.           0          0   \n",
      "1   >sexuality shouldn’t be a grouping category I...           0          0   \n",
      "2     You do right, if you don't care then fuck 'em!           0          0   \n",
      "3                                 Man I love reddit.           0          0   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...           0          0   \n",
      "\n",
      "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
      "0      0          0         0       0          0          0       0  ...   \n",
      "1      0          0         0       0          0          0       0  ...   \n",
      "2      0          0         0       0          0          0       0  ...   \n",
      "3      0          0         0       0          0          0       0  ...   \n",
      "4      0          0         0       0          0          0       0  ...   \n",
      "\n",
      "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0     0            0         0      0            0       0        0        1   \n",
      "1     0            0         0      0            0       0        0        0   \n",
      "2     0            0         0      0            0       0        0        0   \n",
      "3     1            0         0      0            0       0        0        0   \n",
      "4     0            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "Total samples after cleaning: 211225\n"
     ]
    }
   ],
   "source": [
    "# Keep only relevant columns: 'text' + emotion labels\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
    "    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
    "    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
    "    'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "# Select only 'text' and emotion labels\n",
    "clean_df = full_df[['text'] + emotion_labels]\n",
    "\n",
    "# Save the cleaned dataset\n",
    "clean_df.to_csv(\"goemotions_cleaned.csv\", index=False)\n",
    "\n",
    "# Display cleaned dataset info\n",
    "print(clean_df.head())\n",
    "print(f\"Total samples after cleaning: {len(clean_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a91b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with more than 2 emotions: 4807\n"
     ]
    }
   ],
   "source": [
    "emotion_counts = clean_df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Find rows where more than 2 emotions are marked\n",
    "more_than_2_emotions = (emotion_counts > 2).sum()\n",
    "\n",
    "print(f\"Total rows with more than 2 emotions: {more_than_2_emotions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e69caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after removing multi-emotion rows: 206418\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where more than 2 emotions are marked\n",
    "df = clean_df[emotion_counts <= 2].reset_index(drop=True)\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "df.to_csv(\"goemotions_filtered.csv\", index=False)\n",
    "\n",
    "# Print updated dataset info\n",
    "print(f\"Total samples after removing multi-emotion rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d1bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with more than 2 emotions: 0\n"
     ]
    }
   ],
   "source": [
    "emotion_counts = df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Find rows where more than 2 emotions are marked\n",
    "more_than_2_emotions = (emotion_counts > 2).sum()\n",
    "\n",
    "print(f\"Total rows with more than 2 emotions: {more_than_2_emotions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac03cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     emotion\n",
      "0                                    That game hurt.     sadness\n",
      "1   >sexuality shouldn’t be a grouping category I...  admiration\n",
      "2     You do right, if you don't care then fuck 'em!     neutral\n",
      "3                                 Man I love reddit.        love\n",
      "4  [NAME] was nowhere near them, he was by the Fa...     neutral\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-label emotions into a single \"emotion\" column\n",
    "df['emotion'] = df[emotion_labels].idxmax(axis=1)\n",
    "\n",
    "# Keep only 'text' and 'emotion' columns\n",
    "df = df[['text', 'emotion']]\n",
    "\n",
    "# Save the final dataset\n",
    "df.to_csv(\"goemotions_final.csv\", index=False)\n",
    "\n",
    "# Print sample rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f85e7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                                    that game hurt.   \n",
      "1  sexuality shouldnt be a grouping category it m...   \n",
      "2       you do right, if you dont care then fuck em!   \n",
      "3                                 man i love reddit.   \n",
      "4  name was nowhere near them, he was by the falcon.   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                                [that, game, hurt.]   \n",
      "1  [sexuality, shouldnt, be, a, grouping, categor...   \n",
      "2  [you, do, right,, if, you, dont, care, then, f...   \n",
      "3                            [man, i, love, reddit.]   \n",
      "4  [name, was, nowhere, near, them,, he, was, by,...   \n",
      "\n",
      "                                           token_ids  \n",
      "0                                     [8, 172, 3563]  \n",
      "1  [5344, 597, 19, 4, 8416, 10137, 10, 138, 6, 33...  \n",
      "2      [6, 44, 868, 32, 6, 33, 321, 109, 207, 12045]  \n",
      "3                                 [175, 2, 48, 1216]  \n",
      "4    [11, 15, 3217, 889, 1106, 31, 15, 85, 1, 31994]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 📌 Step 1: Text Cleaning (Keep Important Punctuation)\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9!?.,\\s]', '', text)  # Keep only letters, numbers, ! ? . ,\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# 📌 Step 2: Tokenization (Splitting words)\n",
    "df['tokens'] = df['text'].apply(lambda x: x.split())\n",
    "\n",
    "# 📌 Step 3: Build Vocabulary\n",
    "all_words = [word for tokens in df['tokens'] for word in tokens]\n",
    "word_freq = Counter(all_words)\n",
    "vocab = {word: idx+1 for idx, (word, _) in enumerate(word_freq.most_common())}\n",
    "\n",
    "# 📌 Step 4: Convert Tokens to Indexes\n",
    "df['token_ids'] = df['tokens'].apply(lambda tokens: [vocab[word] for word in tokens if word in vocab])\n",
    "\n",
    "# Print sample\n",
    "print(df[['text', 'tokens', 'token_ids']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1106da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile max length: 24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the 95th percentile sequence length\n",
    "sequence_lengths = [len(seq) for seq in df['token_ids'].tolist()]\n",
    "max_length = int(np.percentile(sequence_lengths, 95))\n",
    "\n",
    "print(f\"95th percentile max length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a421ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Apply padding (truncate longer sequences, pad shorter ones)\n",
    "tokenized_padded = pad_sequences(df['token_ids'].tolist(), \n",
    "                                 maxlen=max_length, \n",
    "                                 padding='post',  # Pad at the end\n",
    "                                 truncating='post')  # Truncate at the end\n",
    "\n",
    "# Convert labels to NumPy array\n",
    "labels = np.array(df['emotion'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
